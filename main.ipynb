{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "**Purpose:** To extract and analyse data from the tables on [List of old growth forests](https://en.wikipedia.org/wiki/List_of_old-growth_forests) Wikipedia article using `BeautifulSoup` in Python. This dataset contains loads of data inconsistencies, which will need to be dealt with properly.\n",
    "\n",
    "![Example of Data in this Dataset with Inconsistencies](wikipedia.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking if We Can Scrape\n",
    "\n",
    "Checking if the website allows for scraping by reading its `robots.txt` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.robotparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def can_scrape(url: str, user_agent: str = \"*\") -> bool:\n",
    "    # create an instance of robot parser\n",
    "    rp = urllib.robotparser.RobotFileParser()\n",
    "\n",
    "    # parse the robots.txt file on the website\n",
    "    rp.set_url(url + \"/robots.txt\")\n",
    "    rp.read()\n",
    "\n",
    "    # check if scraping is allowed for the given user agent\n",
    "    return rp.can_fetch(user_agent, url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "website_url = \"https://www.wikipedia.org\"\n",
    "user_agent = \"*\"\n",
    "\n",
    "can_scrape(website_url, user_agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Webscraping Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requesting Webpage and Creating BeautifulSoup Object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Importing libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Define custom function for getting HTML page, calling it on the url and saving it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_html(url, path):\n",
    "    response = requests.get(url)\n",
    "    with open(path, \"w\", encoding = \"utf-8\") as f:\n",
    "        f.write(response.text)\n",
    "\n",
    "url = \"https://en.wikipedia.org/wiki/List_of_old-growth_forests\"\n",
    "\n",
    "get_html(url, path=\"HTML/wiki_old_forrest.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Create `BeautifulSoup` object and extract all tables from `wikitable sortable` classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"HTML/wiki_old_forrest.html\", \"r\", encoding = \"utf-8\") as f:\n",
    "    html = f.read()\n",
    "\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "tables = soup.find_all(\"table\", attrs={\"class\": \"wikitable sortable\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Necessary Metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Extract the table names based on previous headers in the HTML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Africa', 'Asia', 'Australia', 'Europe', 'Canada', 'United States', 'Central America', 'Caribbean', 'South America'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {}\n",
    "for table in tables:\n",
    "    headings = table.find_previous([\"h2\", \"h3\"]).text\n",
    "    data[headings] = table\n",
    "\n",
    "data.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Extract the headers of tables from the first row, using the Australia table as template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Country',\n",
       " 'Area',\n",
       " 'Old-growth extent',\n",
       " 'WWF ecoregion',\n",
       " 'Old-growth forest type']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table = data[\"Australia\"]\n",
    "first_row = table.tr\n",
    "\n",
    "columns = []\n",
    "for td in first_row:\n",
    "    if td.text.strip() != \"\":\n",
    "        columns.append(td.text.strip())\n",
    "\n",
    "columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. QUICKFIX: There is a missing td in United States table (lol), so in order for everything to work we need to REMOVE THE WHOLE DATASET."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data[\"United States\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Africa', 'Asia', 'Australia', 'Europe', 'Canada', 'Central America', 'Caribbean', 'South America'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Functions for Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Function that extracts row data and return a dictionary column names as keys, and corresponding table data as values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_row_data(columns, row):\n",
    "    row_data = {}\n",
    "    table_cells = row.find_all(\"td\")\n",
    "    \n",
    "    for i in range(len(table_cells)):\n",
    "        row_data[columns[i]] = table_cells[i]\n",
    "\n",
    "    return row_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Function that formats the row data dictionary. It does several functions:\n",
    "\n",
    "- Formats cells with whitespace (empty cells) with `No data` string\n",
    "- Replace dead link `<a>` tags with their text and removes citation links\n",
    "- Formats inconsistent `Old-growth extent` data by extracting numeral values with units using `RegEx`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_row_data(row: dict):\n",
    "\n",
    "    for k in row.keys():\n",
    "        val = row[k]\n",
    "\n",
    "        if re.match(\"\\s\", val.text):\n",
    "            row[k] = \"No data\"\n",
    "\n",
    "        links = val.find_all(\"a\")\n",
    "\n",
    "        for l in links:\n",
    "            # Replace dead link <a> tags with their text\n",
    "            if l.get(\"title\") is not None and \"(page does not exist)\" in l.get(\"title\"):\n",
    "                l.replace_with(l.text)\n",
    "\n",
    "            # Remove citation links\n",
    "            if \"cite\" in l.get(\"href\"):\n",
    "                l.parent.decompose()\n",
    "\n",
    "        if k == \"Old-growth extent\" and row[k] != \"No data\":\n",
    "            data = row[k].text.strip()\n",
    "\n",
    "            # format space to proper unicode character\n",
    "            data = data.replace(\"\\xa0\", \" \")\n",
    "            \n",
    "            # finds 2,000 | 7,800,000 units\n",
    "            data = re.search(\"\\d+(?:,\\d{3})*(?:\\.\\d*)? (?:hectares|square kilometres|ha|acres)\", data).group()\n",
    "\n",
    "            parent = row[k].parent\n",
    "            row[k].decompose()\n",
    "\n",
    "            new_tag = soup.new_tag(\"td\")\n",
    "            new_tag.string = data\n",
    "            parent.append(new_tag)\n",
    "\n",
    "            row[k] = new_tag\n",
    "\n",
    "    return row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Function that extracts `tr` tags from tables, removes the header row, and processes each row via `extract_row_data` and `clean_row_data` functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_table_data(columns, table):\n",
    "    table_data = []\n",
    "\n",
    "    rows = table.find_all(\"tr\")\n",
    "    rows.pop(0)\n",
    "\n",
    "    for r in rows:\n",
    "        r = extract_row_data(columns=columns, row=r)\n",
    "        r = clean_row_data(r)\n",
    "        table_data.append(r)\n",
    "\n",
    "    return table_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Function that iterates through the tables stored in `data` dictionary via its keys and calls `prepare_table_data` on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_all_tables(columns, data):\n",
    "    for k in data.keys():\n",
    "        data[k] = prepare_table_data(columns, data[k])\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = prepare_all_tables(columns, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question: How many of the listed forests are in France?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "europe = data[\"Europe\"]\n",
    "france = [r for r in europe if \"France\" in r[\"Country\"].text]\n",
    "len(france)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question: How many of the listed forests are in Tasmania?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([r for r in data[\"Australia\"] if \"Tasmania\" in r[\"Area\"].text])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question: In tasmania, of those that have data, what is the total area of these?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total area for Tasmania forests: 200100.0 ha\n"
     ]
    }
   ],
   "source": [
    "australia = data[\"Australia\"]\n",
    "tasmania = [r for r in data[\"Australia\"] if \"Tasmania\" in r[\"Area\"].text]\n",
    "tasmania_area_data = [r for r in tasmania if r[\"Old-growth extent\"] != \"No data\"]\n",
    "\n",
    "total = 0\n",
    "for r in tasmania_area_data:\n",
    "    area = r[\"Old-growth extent\"].text\n",
    "    \n",
    "    # remove , from numbers\n",
    "    area = area.replace(\",\", \"\")\n",
    "\n",
    "    # extracts digits\n",
    "    val = re.search(\"\\d*\", area).group()\n",
    "    val = float(val)\n",
    "\n",
    "    # normalise data to common units\n",
    "    if \"square kilometres\" in area:\n",
    "        val = val * 100\n",
    "\n",
    "    total += val\n",
    "\n",
    "print(\"Total area for Tasmania forests:\", total, \"ha\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question: From the data of Bulgaria's forests, what is the proportion of Bulgaria's total area that is covered by these?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To answer this question we need to find Bulgaria's total area. We will do it by scraping Bulgaria's wikipedia page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Isolate Old Growth data that is in Bulgaria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "bulgaria_rows = []\n",
    "for row in data[\"Europe\"]:\n",
    "    if row[\"Country\"].text.strip() == \"Bulgaria\":\n",
    "        bulgaria_rows.append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Extract Bulgaria's wikipedia page via the link contained in `Area` columns and use it to parse the website and create a `BeautifulSoup` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the link of the bulgaria article\n",
    "bulgaria_link = \"https://wikipedia.org\" + bulgaria_rows[0][\"Country\"].a[\"href\"]\n",
    "\n",
    "get_html(bulgaria_link, path=\"HTML/bulgaria.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<title>Bulgaria - Wikipedia</title>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_html(bulgaria_link, path=\"HTML/bulgaria.html\")\n",
    "with open(\"HTML/bulgaria.html\", \"r\", encoding = \"utf-8\") as f:\n",
    "    html_bulgaria = f.read()\n",
    "    \n",
    "bulgaria_soup = BeautifulSoup(html_bulgaria, \"html.parser\")\n",
    "bulgaria_soup.title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Create a function that extracts total Bulgaria's area from the Wikipedia's article quick info table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'110,993.6[3]\\xa0km2 (42,854.9\\xa0sq\\xa0mi) (103rd)'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_bulgaria_area(tag):\n",
    "    return tag.name == \"td\" and 'km' in tag.text and 'Total' in tag.parent.text\n",
    "\n",
    "km_tags = [t.text for t in bulgaria_soup.find_all(get_bulgaria_area)]\n",
    "\n",
    "area_tag = km_tags[0]\n",
    "area_tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Extract the number of kilometres squared using `RegEx` anc convert it to hectares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_area = re.search(\"\\d+(?:,\\d{3})*(?:\\.\\d*)?\", area_tag).group()\n",
    "b_area = float(b_area.replace(',', ''))\n",
    "\n",
    "b_area = b_area * 100 # transforms km2 to ha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Extract the size of `Old Growth Extent` and calculate its percentage from the total Bulgaria's area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of bulgarian land area accounted for old growth: 9.9747%\n"
     ]
    }
   ],
   "source": [
    "forest_total = 0\n",
    "for row in bulgaria_rows:\n",
    "    forest_data = row['Old-growth extent'].text\n",
    "    forest_data = re.search(\"\\d+(?:,\\d{3})*(?:\\.\\d*)?\", forest_data).group()\n",
    "    forest_data = float(forest_data.replace(',', ''))\n",
    "\n",
    "    forest_total += forest_data\n",
    "\n",
    "print(f'Percentage of bulgarian land area accounted for old growth: {round((forest_total / b_area)*100, 4)}%')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
